# ğŸš€ Dynamic Data Ingestion and Storage in HDFS with Automated Hive Integration. 
Technologies: Python, API, HDFS, Hive, Shell scripting.

This project showcases an efficient and scalable data ingestion pipeline leveraging the Hadoop ecosystem, specifically HDFS and Apache Hive, to process dynamic U.S. Census data. It simulates real-world big data workflows by ingesting structured CSV data into distributed storage, enabling SQL-based querying through Hive, and laying the groundwork for automated, production-grade data refreshes. This repository reflects my hands-on learning journey in Data Engineering, focusing on building robust, future-ready data infrastructure.
Automated data ingestion into HDFS for fault-tolerant storage.
Schema-on-read optimization via HiveQL for ad-hoc analytics.
Built with extensibility in mind, using HDFS file formats and HiveQL compatible with common Big Data tools.

As part of my Data Engineering specialization, this project mirrors industry practices in distributed data warehousing, emphasizing:
âœ… Modularity (reusable code for future datasets).
âœ… Performance (partitioning, compression in Hive).
âœ… Reproducibility (documented orchestration steps).

Impact: A foundation for building real-time or batch pipelines with enterprise-grade reliability.
---

## ğŸ“Œ Project Objective

Design and implement a robust data ingestion pipeline that:

- Ingests raw census data from a CSV file.
- Uploads it into the Hadoop Distributed File System (HDFS).
- Creates a Hive table with the correct schema.
- Loads and verifies data integrity.
- (Optionally) Automates the data refresh process using a scheduled Python script.

---

## ğŸ§  Tech Stack

- **Hadoop HDFS** â€“ Distributed data storage  
- **Apache Hive** â€“ Data warehousing and SQL querying  
- **Beeline CLI** â€“ Hive interface for query execution  
- **MySQL Metastore** â€“ Hive schema storage  
- **VS Code** â€“ Development environment
- **CentOs9-VMware Workstation** - MobaXterm (SSH Terminal) for Linux based operations.  
- **GitHub** â€“ Version control and collaboration  

---

## ğŸ“‹ Completed Steps to Accomplish Project Agenda

| Step | Description | Status |
|------|-------------|--------|
| 1ï¸âƒ£ | Downloaded census data CSV: `hu-est2001-cty_fips.csv` from Census Bureau | âœ”ï¸ |
| 2ï¸âƒ£ | Inspected schema using `head -5` â€” confirmed 8 columns, comma-delimited, with headers | âœ”ï¸ |
| 3ï¸âƒ£ | Uploaded the CSV to HDFS: `/user/talam/usgov_censuss-data/hu-est2001_cty_fips.csv` | âœ”ï¸ |
| 4ï¸âƒ£ | Started required services: MySQL Metastore, Hive Metastore, HiveServer2, Beeline | âœ”ï¸ |
| 5ï¸âƒ£ | Created Hive database: `usg_censusdb` | âœ”ï¸ |
| 6ï¸âƒ£ | Created Hive table `us_population` with proper 8-column schema | âœ”ï¸ |
| 7ï¸âƒ£ | Loaded data from HDFS into Hive table | âœ”ï¸ |
| 8ï¸âƒ£ | Verified data load & integrity with queries | âœ”ï¸ |

---

ğŸ’» Tools & Setup Used
Tools	| Role  
(Apache) - Hadoop	HDFS setup & file handling.
(Apache Hive)	- Query engine and data warehouse.
(MySQL) -	Hive Metastore backend.
(Beeline)	- Interface to execute Hive queries.

ğŸ Future Scope
Add dynamic ingestion automation using Python,
Integrate monitoring/logging for job execution,
Transition this pipeline for larger datasets and file formats like Parquet.

## ğŸ“‚ Hive Table Schema

The table `us_population` was created with the following schema:

```sql
CREATE TABLE us_population (
  fips_state INT,
  fips_county INT,
  region INT,
  division INT,
  estimate_2001 BIGINT,
  estimate_2000 BIGINT,
  census_2000 BIGINT,
  area_name STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

ğŸ“Š **Sample Query Output**
SELECT * FROM us_population LIMIT 10;
fips_state	fips_county	region	division	estimate_2001	estimate_2000	census_2000	area_name
1	0	3	6	1996827	1969682	1963711	Alabama
1	1	3	6	18036	17728	17662	Autauga County

ğŸ” Optional Next Step: Automate Future Data Refreshes
A Python-based automation script (to be added) will help:
Fetch the latest CSV from a source or folder.
Upload it to HDFS.
Overwrite or Append the Hive table with new data.
Schedule this pipeline via cron or Airflow.
This will enable a production-style refresh mechanism, simulating dynamic data pipelines in big data projects.
